# 1.学习率

根据经验，可以从以下几个数值开始试验α的值，<font color='red'>0.001 ,0.003, 0.01, 0.03, 0.1, 0.3, 1, …α初始值位0.001</font>, 不符合预期乘以3倍用0.003代替，不符合预期再用0.01替代，如此循环直至找到最合适的α

一般而言，较小的学习率将需要更多的训练时期。 相反，较大的学习率将需要较少的训练时间。 此外，考虑到误差梯度的噪声估计，较小的批量大小更适合于较小的学习率

## 在pytorch中，如何使用使用学习率调度器来自适应地调整学习率。

在PyTorch中，可以使用学习率调度器（learning rate scheduler）来自适应地调整学习率。学习率调度器可以根据训练的进展情况动态地减小或增大学习率，以提高训练的效率和性能。以下是使用学习率调度器的一般步骤：

1. **导入必要的库**：

```python
import torch
import torch.optim as optim
from torch.optim import lr_scheduler
```

2. **定义优化器和初始学习率**：

首先，定义一个优化器（如SGD、Adam等）以及初始学习率。例如：

```python
optimizer = optim.SGD(model.parameters(), lr=0.1)
```

3. **定义学习率调度器**：

选择合适的学习率调度器，PyTorch提供了一些常用的调度器，例如`lr_scheduler.StepLR`、`lr_scheduler.MultiStepLR`、`lr_scheduler.ReduceLROnPlateau`等。以下是使用`StepLR`调度器的示例：

```python
scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
```

这里，`step_size`表示每隔多少个epoch将学习率降低，`gamma`表示学习率降低的倍数。

4. **在训练循环中更新学习率**：

在每个epoch或每个训练步骤中，调用学习率调度器的 `step()` 方法来更新学习率。例如：

```python
for epoch in range(num_epochs):
    # 训练代码
    optimizer.step()
    scheduler.step()
```

在这个示例中，`optimizer.step()`用于更新模型参数，而`scheduler.step()`用于更新学习率。

5. **根据需要监控学习率的变化**：

如果你想监控学习率的变化，可以使用`scheduler.get_lr()`方法来获取当前学习率的值。例如：

```python
current_lr = scheduler.get_lr()
print(f"Current learning rate: {current_lr}")
```

这样，你就可以在训练过程中了解学习率的动态变化。

请注意，不同的学习率调度器有不同的用法和参数设置，具体的选择取决于你的任务需求和训练策略。学习率调度器的使用有助于更好地控制训练过程中的学习率，从而提高模型的训练效果。

## 2.权重衰减

weight decay（权值衰减）的使用既不是为了提高收敛精确度，也不是为了提高收敛速度，其<font color='red'>最终目的是防止过拟合</font>

<font color='red'>保持权重在一个较小在的值，避免梯度爆炸</font>。

一般，

1. `weight_decay` = 1e-4

2. `weight_decay` = 1e-6
3. 更多的，base_lr与weight_decay相差大概是两到三个数量级

# 2.Momentum

<font color='red'>momentum是SGD优化器特有的参数  一般设定为0.9</font>

在使用 `torch.optim.SGD` （随机梯度下降）优化器时，<font color='red'>`momentum` 参数可以帮助加速收敛和提高模型性能</font>。`momentum` 的值通常介于 0（无动量）和 1（最大动量）之间。常见的建议是将动量设置在 0.5 到 0.9 之间。

一个常用且在实践中效果很好的动量值是 **0.9**。这个值在许多深度学习任务中已经被证明是有效的。然而，根据您的具体问题和数据集，可能需要尝试不同的动量值以获得最佳性能。

请注意，设置过高的动量值可能导致优化器在搜索最优解时振荡。因此，选择适当的动量值非常重要。如果您没有足够的经验选择最佳动量值，可以使用网格搜索、随机搜索或贝叶斯优化等超参数优化技术来找到适合您问题的最佳动量值。

# 3.Adam和SGD的选用

```python
#   optimizer_type  使用到的优化器种类，可选的有adam、sgd
#                   当使用Adam优化器时建议设置  Init_lr=1e-3
#                   当使用SGD优化器时建议设置   Init_lr=1e-2
#   momentum        优化器内部使用到的momentum参数
#   weight_decay    权值衰减，可防止过拟合
#                   adam会导致weight_decay错误，使用adam时建议设置为0。
```

- SGD在下列情景中可能更胜一筹：
  - 数据集非常大并且可能无法全部装入内存时，因为SGD每次只需使用部分数据进行梯度计算。
  - 在深度学习问题中，虽然SGD收敛速度慢，但有时会比Adam获得更好的最终性能。
- Adam在下列情景中可能更胜一筹：
  - 在神经网络和深度学习中，特别是在处理复杂非凸优化问题时，Adam通常可以带来更好的结果。
  - 当处理稀疏数据或者需要快速收敛时，Adam优于SGD，因为Adam对不同的参数具有自适应的学习率

[优化方法总结以及Adam存在的问题(SGD, Momentum, AdaDelta, Adam, AdamW，LazyAdam)-CSDN博客](https://blog.csdn.net/yinyu19950811/article/details/90476956)

# 4.损失函数的选择

对于回归问题，应该使用回归损失函数，例如均方误差损失（Mean Squared Error Loss, `torch.nn.MSELoss`）或平滑L1损失（Smooth L1 Loss, `torch.nn.SmoothL1Loss`），而不是分类问题的损失函数（如交叉熵损失）（`torch.CrossEntropyLoss`）

[损失函数（lossfunction）的全面介绍（简单易懂版）-CSDN博客](https://blog.csdn.net/weixin_57643648/article/details/122704657?ops_request_misc=%7B%22request%5Fid%22%3A%22169665113016800213033652%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=169665113016800213033652&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-122704657-null-null.142^v95^insert_down1&utm_term=损失函数&spm=1018.2226.3001.4187)

# 5.提高神经网络训练准确率和拟合速率

[卷积神经网络提高准确率（shuffle,优化器，batchsize,权重初始化）通过某次实际CNN调参过程_cnn提高准确率_执契的博客-CSDN博客](https://blog.csdn.net/qq_36187544/article/details/90478051?ops_request_misc=&request_id=&biz_id=102&utm_term=shuffle对神经网络训练的影响&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-5-90478051.142^v90^control_2,239^v3^insert_chatgpt&spm=1018.2226.3001.4187)
