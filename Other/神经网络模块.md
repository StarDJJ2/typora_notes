# 1.CSPnet

**设计CSPNet的主要目的是使该架构能够实现更丰富的梯度组合，同时减少计算量。这个目的是通过将基础层的特征图分割成两部分，然后通过提出的跨阶段分层（cross-stage hierarchy）结构进行合并来实现的**。**我们的主要概念是通过分割梯度流，使梯度流在不同的网络路径中传播。通过这种方式，我们证实了通过switching concatenation and transition steps步骤，传播的梯度信息可以有很大的相关性差异**。此外，CSPNet可以大大减少计算量，提高推理速度以及精度

# 2.SCA

##### 2022 AQE-Net A Deep Learning Model for Estimating Air Quality of Karachi City from Mobile Images

![bandicam 2023-07-24 20-13-29-461](C:\Users\25075\Desktop\bandicam 2023-07-24 20-13-29-461.png)



# 3.Dilation Convolution

##### 2022 URNet: A U-Net based residual network for image dehazing

[(2条消息) Dilated Convolutions 空洞卷积 pytorch版_空洞卷积代码_嘿芝麻的博客-CSDN博客](https://blog.csdn.net/zw__chen/article/details/85007519?ops_request_misc=%7B%22request%5Fid%22%3A%22169025303916800225534683%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=169025303916800225534683&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-21-85007519-null-null.142^v91^insertT0,239^v3^insert_chatgpt&utm_term=DILATED CONVOLUTIONS&spm=1018.2226.3001.4187)

空洞卷积
想要获取较大感受野需要用较大的卷积核或池化时采用较大的stride，对于前者计算量太大，后者会损失分辨率。然而想要对图片提取的特征具有较大的感受野，并且又想让特征图的分辨率不下降太多(分辨率损失太多会丢失许多关于图像边界的细节信息)，但这两个是矛盾的。而空洞卷积就是用来解决这个矛盾的。即可让其获得较大感受野，又可让分辨率不损失太多。

https://blog.csdn.net/sleepinghm/article/details/120422062

空洞卷积与标准卷积的融合模块：

```python
import torch
import torch.nn as nn

class CombinedBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(CombinedBlock, self).__init__()
        
        self.standard_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels/2, kernel_size=3, padding=1),
            nn.ReLU(),
        )
        
        self.dilated_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channel/2, kernel_size=3, padding=2, dilation=2),
            nn.ReLU(),
        )
        
    def forward(self, x):
        out_standard = self.standard_conv(x)
        out_dilated = self.dilated_conv(x)
        combined_output = torch.cat((out_standard, out_dilated), dim=1)
        return combined_output

# 创建示例输入
batch_size = 16
input_channels = 3
input_height = 64
input_width = 64
dummy_input = torch.randn(batch_size, input_channels, input_height, input_width)

# 创建块实例并应用
in_channels = input_channels
out_channels = 64
combined_block = CombinedBlock(in_channels, out_channels)
output = combined_block(dummy_input)

# 打印输出形状
print("Output shape:", output.shape)


```



# 4.Pixel Attention

Pixel Attention通常是对每一个像素在所有通道上进行注意力评分，然后使用这个评分来重新调整原始输入。这是一种更细粒度的注意力机制，因为它关注的是每一个单独的像素，而不是整个通道或者特定区域。

这是一个使用PyTorch实现的Pixel Attention的例子：

```python
import torch
import torch.nn as nn

class PixelAttention(nn.Module):
    def __init__(self, in_channels):
        super(PixelAttention, self).__init__()
        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)

    def forward(self, x):
        maps = torch.sigmoid(self.conv(x))
        return maps * x
```

在这个PixelAttention模块中：

1. `self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)`: 这行代码定义了一个1x1的卷积层，它将输入的通道数减少到1，所以这个卷积层实际上是在执行通道注意力的计算，它会为每个像素计算一个注意力分数。

2. `maps = torch.sigmoid(self.conv(x))`: 这行代码首先通过1x1卷积层计算每个像素的注意力分数，然后通过sigmoid函数将注意力分数限制在0到1之间。

3. `return maps * x`: 这行代码将原始输入和注意力分数进行逐元素乘法，这样每个像素都会被赋予一个权重，这个权重就代表了模型对这个像素的注意力。

   ## Resnet中加入注意力机制

   [(4条消息) 注意力机制在CNN中使用总结_cnn注意力机制放到哪里_AI浩的博客-CSDN博客](https://blog.csdn.net/hhhhhhhhhhwwwwwwwwww/article/details/114449042)

## 5.为什么通过Squeeze 操作，从空间维度来进行特征压缩，将h*w*c的特征变成一个1*1*c的特征，可以得到向量某种程度上具有全域性的感受野

在卷积神经网络（Convolutional Neural Networks，CNN）中，接受场（Receptive Field）是指每个神经元在原始输入图像中能"看到"的区域大小。在多层卷积网络中，随着网络层级的加深，每个神经元的接受场会逐渐扩大，能够感知到更大范围的输入信息。因此，深层的神经元具有较大的接受场，能够获取全局的信息。

使用全局平均池化（Global Averaging Pooling，GAP）或全局最大池化（Global Max Pooling）进行特征压缩是一种常见的操作，它可以将输入的特征图从H*W*C压缩到1*1*C。这个过程实际上是在每个通道上对所有的空间位置进行平均或者最大值操作，因此，得到的每个通道的特征值都是对应通道所有空间特征的一个全局性描述。换句话说，这个操作可以使得处理后的特征在空间维度上具有全域的感受野。

这种全局池化操作的一个重要作用是能够抽取全局上下文信息。对于某些任务，如物体分类、场景识别等，全局上下文信息是非常重要的。此外，全局池化操作还有利于降低参数数量，减小模型复杂度，防止过拟合的发生。

因此，通过对原始特征进行全局池化操作，我们可以获得一个具有全局感受野的特征向量，这个特征向量能够代表输入特征在全局范围内的信息。

# 6.模块组成

就拿目标检测模型来说吧，一般的目标检测模型有3大组件，backbone、neck、head。这里的backbone的作用是特征提取，neck的作用是特征融合，head用来做特征分类和回归预测。

再说一下cnn中常用的轻型注意力机制如se、cbam、ca等都是通过改变权重参数增强cnn的局部注意力。提升cnn对局部特征的识别能力。

再把两个综合一下，我只有backbone需要使用注意力机制来加强我对特征的提取能力，别的层都不需要注意力机制。其实neck中有个方法是和注意力机制类似的改善权重的方法的叫ASFF。你可以搜一下看看。


链接：https://www.zhihu.com/question/466708498/answer/1955770547

# 7.瓶颈结构

这种设计通常用于减少模型的计算复杂度和参数数量

比如输入特征图的通道数为`in_planes`，第一个全连接层将通道数下降到`in_planes // ratio`，然后第二个全连接层将通道数再增加回`in_planes`。
这样就形成了一个瓶颈结构，中间层的神经元数量少于输入和输出层。

# 8.深度可分离卷积（Depthwise Separable Convolution）

[深度可分离卷积 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/92134485)

depth-wise卷积虽然参数量少了，但是计算速度却变慢了。因为GPU IO的瓶颈
