# ReLU、Leaky ReLU、PReLU和RReLU的比较

![img](https://imgconvert.csdnimg.cn/aHR0cDovL3AwLmlmZW5naW1nLmNvbS9wbW9wLzIwMTcvMDcwMS9DNTZFNUM2RkNCQjM2RTcwQkE1RUJDOTBDQkQxNDJCQTMyMEIzREY2X3NpemUxOV93NzQwX2gyMTcuanBlZw?x-oss-process=image/format,png)

[ReLU、Leaky ReLU、PReLU和RReLU激活函数_prelu作用_TBYourHero的博客-CSDN博客](https://blog.csdn.net/weixin_41803874/article/details/102949574?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_utm_term~default-12-102949574-blog-51345832.235^v38^pc_relevant_default_base&spm=1001.2101.3001.4242.7&utm_relevant_index=15)



## ReLU 的缺点：

训练的时候很”脆弱”，很容易就”die”了
例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.
如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。

# GELU激活函数

[GELU激活函数介绍和笔记_夜晓岚渺渺的博客-CSDN博客](https://blog.csdn.net/kkxi123456/article/details/122694916?ops_request_misc=%7B%22request%5Fid%22%3A%22169474517516800180623688%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=169474517516800180623688&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-6-122694916-null-null.142^v94^insert_down1&utm_term=GELU&spm=1018.2226.3001.4187)

# 激活函数之间的比较

[深度学习常用激活函数之— Sigmoid & ReLU & Softmax_激活函数relu-CSDN博客](https://blog.csdn.net/Leo_Xu06/article/details/53708647)





## 那我如何知道是ReLU表现不佳导致的模型性能不佳呢？

判断ReLU是否导致模型性能不佳并没有直观的方法，主因是在深度学习中，性能不佳往往可能是多种因素导致的，包括但不限于学习率、优化器、神经网络架构、批大小、过拟合或者欠拟合等等。

 然而，当你的模型经过多次迭代和尝试优化其他超参数后依然无法改善性能，而且模型的训练错误无法收敛，或者训练和验证指标出现较大差距，这可能是死神经元问题导致的。这个时候可以尝试其他类型的激活函数，如LeakyReLU或者ELU。 

实际上，对比不同的激活函数并观察模型的性能对于深度学习来说是常见的做法。多试验几种不同的激活函数，观察模型的训练和验证效果，可能会帮助你更好地理解激活函数在模型性能上的影响。 

总的来说，发现ReLU是否导致模型性能不佳主要是基于观察、经验和多次试验。