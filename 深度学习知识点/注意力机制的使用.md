# 1.适用范围

注意力机制在神经网络中的使用并不局限于只处理浅层或深层特征。其应用范围取决于特定任务的需求和设计者的选择。

1. **用于浅层特征**：在一些任务中，浅层特征（如边缘、纹理等）可能与任务的目标密切相关。在这种情况下，可能会在网络的浅层使用注意力机制，以便更好地捕获这些重要的浅层特征。

2. **用于深层特征**：在其他任务中，可能更关注高级抽象的特征（如对象部分或整体对象）。这时，可能会在网络的深层使用注意力机制，以便更好地捕获这些高级特征。

3. **同时用于浅层和深层特征**：在一些复杂的任务中，可能同时需要关注浅层特征和深层特征。例如，目标检测和语义分割等任务通常需要同时考虑低级细节和高级语义信息。在这种情况下，可能会在网络的多个层级（包括浅层和深层）使用注意力机制。

注意力机制的关键思想是帮助模型更好地关注和利用对当前任务有用的信息。因此，其使用并不局限于特定的层级，而是可以灵活地应用到网络的任何部分，以便最大限度地提高模型的性能。



# 通过 sigmoid 函数将向量归一化为 [0, 1]，生成通道权重

通过sigmoid函数将向量归一化到[0, 1]范围内，通常用于生成通道权重的操作，是为了引入通道注意力（Channel Attention）的机制。通道注意力是一种用于深度学习中的注意力机制，它的目标是为不同通道的特征图分配不同的权重，以便网络可以自适应地选择哪些通道对当前任务最有用。

以下是通过sigmoid函数生成通道权重的一般原理：

1. **特征图的生成**：首先，通过卷积神经网络（CNN）或其他特征提取方法，获得一组特征图（feature maps），每个特征图对应网络中的一个通道。

2. **通道权重的计算**：对于每个通道，通过计算该通道的全局平均池化（global average pooling）或全局最大池化（global max pooling）来获取其重要性。这一步骤将每个通道的信息压缩为一个标量。

3. **通过sigmoid函数归一化**：将池化后的标量输入sigmoid函数，将其归一化到[0, 1]范围内。sigmoid函数的输出表示了每个通道在当前任务中的重要性或权重。

4. **通道加权**：最后，将归一化后的权重与原始特征图相乘，以获取具有权重信息的特征图。这个过程可以看作是动态地给每个通道分配了不同的权重，以便网络更好地适应不同的特征表示需求。

通道注意力的主要优势在于它可以自动学习每个通道的重要性，而不需要手动设计权重。这允许网络根据输入数据的不同特征和任务的不同需求来自适应地调整通道的重要性，从而提高了模型的性能。

通道注意力常常用于视觉任务，如图像分类、目标检测和分割，以提高模型在不同类别或不同图像部分之间的区分能力。类似的注意力机制也可以应用于其他领域，如自然语言处理。

## Sigmoid归一化：

- Sigmoid函数将输入映射到[0, 1]范围内，并且每个通道的输出是独立的。这意味着每个通道的权重系数在[0, 1]之间，不会相互竞争，而是表示每个通道相对于当前任务的贡献或重要性。

- Sigmoid函数的输出通常用于给每个通道分配一个权重，以控制其对特定任务的贡献。这对于多通道特征的权重调整非常有用，例如在注意力机制中，可以根据不同通道的重要性来选择性地聚焦于特定通道。

  <font color='red'>sigmoid函数将输入变换为(0,1)上的输出。它将范围(-inf,inf)中的任意输入压缩到区间(0,1)中的某个值</font>

## Softmax归一化：

- Softmax函数用于多类别分类问题，其输出是一个概率分布，确保所有类别的概率之和等于1。它用于将输入转换为一个概率分布，以便在分类任务中选择一个类别。
- Softmax函数的输出通常不适用于多通道特征的权重分配，因为它会引入<font color='red'>互斥性</font>，即一个通道的权重增加会导致其他通道的权重减小，以保持总和为1。这通常不适用于表征多通道特征的重要性，因为它不允许多个通道同时具有高权重。

在二分类任务时，经常使用sigmoid激活函数。而在处理多分类问题的时候，需要使用softmax函数。

<font color='red'>它的输出有两条规则</font>

- 每一项的区间范围的(0,1)
- 所有项相加的和为1.

# 硬注意力机制和软注意力机制

深度学习中的注意力机制通常可分为三类：<font color='red'>软注意（全局注意）、硬注意（局部注意）和自注意（内注意）</font>

1. Soft/Global Attention(软注意机制)：对每个输入项的分配的权重为0-1之间，也就是某些部分关注的多一点，某些部分关注的少一点，因为对大部分信息都有考虑，但考虑程度不一样，所以相对来说计算量比较大。
2. Hard/Local Attention(硬注意机制)：对每个输入项分配的权重非0即1，和软注意不同，硬注意机制只考虑那部分需要关注，哪部分不关注，也就是直接舍弃掉一些不相关项。优势在于可以减少一定的时间和计算成本，但有可能丢失掉一些本应该注意的信息。
3. Self/Intra Attention（自注意力机制）：对每个输入项分配的权重取决于输入项之间的相互作用，即通过输入项内部的"表决"来决定应该关注哪些输入项。和前两种相比，在处理很长的输入时，具有并行计算的优势

## 自注意力机制

CNN自注意力机制是一种根据两两之间的关系来引入权重的方法，可以用于图像或语音领域。它和CNN的概念类似，都是让模型考虑一个感受野，但是自注意力机制可以让模型自己决定感受野的形状和类型1。它和RNN的不同之处在于，它可以同时处理上下文，而不是仅局限于前文，而且它是并行处理，提高了效率2。它的实现方式是通过对每一个特征层进行全局池化，再到全连接层中找特征之间的联系，最后得到权重划分

## 硬注意力机制

硬注意力是一种离散的注意力机制，它在给定输入序列中选择一个或少数几个特定的元素进行关注。这意味着硬注意力产生一个二进制掩码，用于确定哪些元素是重要的（1）和哪些元素是不重要的（0）

## 软注意力机制

软注意力是一种连续的、平滑的注意力机制，它对输入序列中的每个元素都分配一个权重，而不是仅选择少数元素。这些权重是在[0, 1]范围内的概率分布，它们表示了模型对不同元素的关注程度