# 1.加权

"加权"操作其实就是神经元通过权重来评估每个输入对输出的贡献或重要性，这是神经网络能够学习和逼近复杂函数的关键所在

# 2.权值共享

理解卷积神经网络（Convolutional Neural Network, CNN）中的"权值共享"概念，我们首先要明确卷积操作的含义。在CNN中，卷积层使用一种叫做"过滤器"或"卷积核"的结构来扫描输入数据（例如图像）。过滤器中的每个元素（即权重）都参与了卷积操作。

"权值共享"的含义是，过滤器在移动扫描整个输入数据时，其权值（即过滤器中的元素值）是不变的。无论过滤器在扫描输入数据的哪一部分，使用的都是相同的权重。通过这种方式，CNN能够在各个空间位置寻找和学习相同的特征（例如边缘、角点等局部特征），这使得CNN具有平移不变性，即无论特征在图像的哪个位置，网络都能检测到。

然后，在训练过程中，这些权重会通过反向传播和梯度下降等方法进行更新，以降低网络的损失值。但这种更新是针对整个过滤器的，也就是说，过滤器在权重更新后，其新的权重值会同时应用于扫描整个输入数据，仍然保持权值共享。

所以，当我们说"在训练阶段，过滤器在筛选每个样本时，不改变其权值"时，是指过滤器在扫描单个样本的过程中权值是固定的，这些权值只有在一个完整的前向和反向传播中才会进行更新。

# 3.model.children( )和model.module( )两个函数的区别

在PyTorch中，`model.modules()`和`model.children()`都是用于迭代模型中的层或者模块。然而，它们的遍历范围不同。

1. `model.children()`只返回模型的直接子模块。如果子模块内部还包含着子模块（即嵌套子模块），`model.children()`并不会遍历这些嵌套的子模块。

例如：

```python
import torch
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU()
        )
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.conv(x)
        x = self.fc(x)
        return x

model = MyModel()

# 使用 model.children() 遍历一级子模块
for child in model.children():
    print(child)
```
在这个例子中，`model.children()`将会返回两个子模块 —— `self.conv`（nn.Sequential的实例）和`self.fc`（nn.Linear的实例）。而`nn.Sequential`实例内部的`nn.Conv2d`和`nn.ReLU`模块并不会被遍历到。

2. `model.modules()`返回模型中的所有模块，包括模型自身，直接子模块，以及所有的嵌套子模块。

使用同样的模型：

```python
# 使用 model.modules() 遍历所有模块
for module in model.modules():
    print(module)
```
这个例子中，`model.modules()`将会返回模型自身（`MyModel`的实例）、它的一级子模块（`self.conv`和`self.fc`），以及`self.conv`内部的所有子模块（`nn.Conv2d`和`nn.ReLU`）。

所以在遇到需要对模型内部的所有模块进行操作的情况——比如需要冻结和解冻的参数，或者要注册一些hook函数时，我们会常常使用`model.modules()`方法。