# 1.单参数k拟合

```python
# Sigmoid function for w: w = 1 / (1 + exp(-k * (s - 1)))
def calculate_weighted_sum_sigmoid(params, s, x1, x2):
    k = params
    w = 1 / (1 + np.exp(-k * (s - 1)))  # Modify the sigmoid function
    return w * x1 + (1 - w) * x2

# Objective function to minimize with sigmoid function
def objective_function_sigmoid(params, s, x1, x2, y_actual):
    y_pred = calculate_weighted_sum_sigmoid(params, s, x1, x2)
    return np.sum((y_pred - y_actual)**2)

# Initial guess for parameters k for the sigmoid function
initial_params_sigmoid = [1]

# Minimize the objective function with sigmoid approach
result_sigmoid = minimize(objective_function_sigmoid, initial_params_sigmoid, args=(s, x1, x2, y), method='L-BFGS-B')

# Optimized parameters k for the sigmoid function
optimized_params_sigmoid = result_sigmoid.x
optimized_params_sigmoid

```

这段代码的主要目的是通过优化sigmoid函数的参数k来最小化目标函数，更具体来说,这个目标函数是预测值和实际值的平方差的总和。该代码使用了梯度下降法的一种变种，称为L-BFGS-B。

1. `calculate_weighted_sum_sigmoid(params, s, x1, x2)`: 这个函数返回通过sigmoid函数计算得到的加权和。其中，sigmoid函数是w = 1 / (1 + exp(-k * (s - 1)))，它使用一个参数k（由`params`提供），并且基于此计算一个权重w。之后，这个权重被用来在x1和x2之间进行权重分配。
2. `objective_function_sigmoid(params, s, x1, x2, y_actual)`: 这个函数计算并返回预测值（通过`calculate_weighted_sum_sigmoid`函数计算得到）和实际值之间的平方误差的总和，这是需要最小化的目标函数。
3. `initial_params_sigmoid = [1]`: 这行代码定义了一个初始化的参数k值，用于开始梯度下降的过程。
4. `result_sigmoid = minimize(objective_function_sigmoid, initial_params_sigmoid, args=(s, x1, x2, y), method='L-BFGS-B')`: 这行代码使用**scipy库的minimize函数**，尝试找到最小化目标函数的参数k值。这里，使用的优化算法是L-BFGS-B。
5. `optimized_params_sigmoid = result_sigmoid.x`: 优化结束后，最优化的参数值存储在返回的对象的`x`属性中。这行代码获取并存储了这些优化的参数。

总的来说, 这段代码是通过梯度下降算法找到一个sigmoid函数的最优参数，使得基于这个参数计算出来的预测值与实际值之间的平方误差的和最小。



# 2.双参数a，k拟合

```
import numpy as np
from scipy.optimize import minimize

# Sigmoid function for w: w = 1 / (a + exp(-k * (s - 1)))
def calculate_weighted_sum_sigmoid(params, s, x1, x2):
    a, k = params
    w = 1 / (a + np.exp(-k * (s - 1)))
    return w * x1 + (1 - w) * x2

# Objective function to minimize with sigmoid function
def objective_function_sigmoid(params, s, x1, x2, y_actual):
    y_pred = calculate_weighted_sum_sigmoid(params, s, x1, x2)
    return np.sum((y_pred - y_actual)**2)

# Initial guess for parameters a and k for the sigmoid function
initial_params_sigmoid = [1, 1]

# Minimize the objective function with sigmoid approach
result_sigmoid = minimize(objective_function_sigmoid, initial_params_sigmoid, args=(s, x1, x2, y), method='L-BFGS-B')

# Optimized parameters a and k for the sigmoid function
optimized_params_sigmoid = result_sigmoid.x
optimized_params_sigmoid

```

这段代码和先前的代码非常相似，但在这个版本中，sigmoid函数有两个需要优化的参数（a和k）。这导致了一些小的改变。

1. `calculate_weighted_sum_sigmoid(params, s, x1, x2)`: 这个函数返回一个通过优化后的sigmoid函数计算得到的加权和。sigmoid函数是w = 1 / (a + exp(-k * (s - 1)))，这里多了一个参数a，所以需要对其进行优化。
2. `objective_function_sigmoid(params, s, x1, x2, y_actual)`: 这和之前的代码一样，计算并返回预测值和实际值之间的平方误差和，这是需要最小化的目标函数。
3. `initial_params_sigmoid = [1, 1]`: 这个代码定义了初始化的参数a和k的值，这个值将用于开始梯度下降的过程。注意这一次有两个初始化参数而非一个。
4. `result_sigmoid = minimize(objective_function_sigmoid, initial_params_sigmoid, args=(s, x1, x2, y), method='L-BFGS-B')`: 这个代码使用scipy的minimize函数，尝试找到最小化目标函数的参数a和k的值。同样的，优化算法是L-BFGS-B。
5. `optimized_params_sigmoid = result_sigmoid.x`: 在优化结束后，最优的参数值存储在返回的对象的`x`属性中。这一行代码获取并存储了这些优化后的参数。

总的来说，这段代码通过梯度下降算法找到一个sigmoid函数的最优参数a和k，使得基于这些参数计算出来的预测值和实际值之间的平方误差的和最小。



# 算法`L-BFGS`的步骤如下所示

![img](https://pic4.zhimg.com/v2-42e4ed45ef5f2a8a96739bbeb04e34ef_r.jpg)



## 算法`L-BFGS`介绍：

[L-BFGS-B 算法是一种用于求解无约束或有界约束优化问题的拟牛顿算法，它是基于 BFGS 算法的改进，利用有限的内存空间来近似黑塞矩阵或其逆矩阵，从而降低了计算和存储的代价。L-BFGS-B 算法可以有效地处理大规模的非线性优化问题，特别是在机器学习和数据挖掘领域有广泛的应用](https://zhuanlan.zhihu.com/p/109253217)[1](https://zhuanlan.zhihu.com/p/109253217)[2](https://zhuanlan.zhihu.com/p/514576143)。

[L-BFGS-B 算法的基本思想是只保存最近的 m 次迭代信息，即 s_k 和 y_k，其中 s_k = x_ {k+1}-x_k，y_k = \nabla f (x_ {k+1}) - \nabla f (x_k)。然后通过两层循环的方式，利用这些信息来计算搜索方向 p_k = -H_k \nabla f_k，其中 H_k 是黑塞矩阵的近似逆矩阵。如果有界约束，L-BFGS-B 算法还会使用投影梯度法来处理不可行点，保证每次迭代的点都在可行域内](https://github.com/bgranzow/L-BFGS-B)[3](https://github.com/bgranzow/L-BFGS-B)[4](https://blog.csdn.net/m0_48945359/article/details/124448662)。

[L-BFGS-B 算法的优点是收敛速度快，鲁棒性强，内存消耗小，适用于大规模问题。它的缺点是需要调整的参数较多，如 m 的取值，一维搜索的步长，以及有界约束的容忍度等。另外，L-BFGS-B 算法不能处理非凸问题，也不能处理含有 L1 正则化项的问题，需要使用其他的变种算法，如 OWL-QN 算法](https://ww2.mathworks.cn/help/deeplearning/ref/deep.lbfgsstate.html)[5](https://ww2.mathworks.cn/help/deeplearning/ref/deep.lbfgsstate.html)。

​              