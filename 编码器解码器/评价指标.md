# PSNR**（Peak Signal to Noise Ratio）**峰值信噪比

PSNR的单位是dB，数值越大表示失真越小

# SSIM**（structural similarity）**结构相似性

SSIM取值范围[0,1]，值越大，表示图像失真越小.

[图像的峰值信噪比（PSNR）的计算方法_图像的信噪比怎么计算-CSDN博客](https://blog.csdn.net/xrinosvip/article/details/88569111)



# L1loss和MAEloss之间的区别

L1 损失和 MAE（Mean Absolute Error）是两种常见的回归任务中使用的损失函数，它们在数学定义上有些不同，但通常在实践中用途相似。以下是它们之间的主要区别：

1. **定义**：
   
   - L1 损失是预测值和目标值之间绝对差的平均值。数学表达式为：
     $$
     L1Loss = \frac{1}{n}\sum_{i=1}^{n} |y_i - \hat{y}_i|
     $$
   
   - MAE 是平均绝对误差，也是预测值和目标值之间绝对差的平均值。数学表达式为：
     $$
     MAE = \frac{1}{n}\sum_{i=1}^{n} |y_i - \hat{y}_i|
     $$
   
2. **Robustness（鲁棒性）**：
   
   - L1 损失对异常值（outliers）比 MAE 更敏感。因为 L1 损失在计算过程中采用了绝对值，所以异常值对它的影响会更大。
   - MAE 对异常值的影响较小，因为它在计算过程中不受绝对值的影响。
   
3. **梯度（Gradient）**：
   
   - 在优化过程中，L1 损失的梯度在原点处不连续。但是，如果考虑在原点附近的梯度，它们是稳定的。这使得在某些优化算法中（例如随机梯度下降）可能会出现挑战。
   - MAE 损失的梯度在所有地方都是连续的，这使得在优化过程中更容易处理。
   
4. **应用**：
   
   - L1 损失通常用于稀疏信号建模或稀疏系数估计，以及一些正则化方法中。
   - MAE 通常用于回归任务的损失函数，尤其是在需要对异常值具有一定鲁棒性的情况下。

总的来说，它们之间的选择取决于具体的问题情况和对异常值的敏感度。如果你的数据中存在较多的异常值，并且你希望模型对这些异常值具有一定的鲁棒性，那么可能会倾向于使用 MAE 损失。如果异常值对你的问题不是很重要，或者你希望模型更敏感地对待目标值与预测值之间的小差异，那么可以考虑使用 L1 损失。



# L2loss

L2 Loss 和 Smooth L1 Loss 是两种常见的回归任务中使用的损失函数，它们在处理异常值和梯度特性上有一些不同。以下是它们的介绍：

1. **L2 Loss（平方损失）**：
   - L2 Loss 是预测值和目标值之间差值的平方的平均值。数学表达式为：\[ 
     $$
     L2Loss = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2
     $$
   - L2 Loss 对异常值非常敏感，因为差值被平方，这意味着较大的异常值会对损失函数产生较大的影响。
   - L2 Loss 在训练过程中可能会导致梯度消失或梯度爆炸的问题，尤其是在深度神经网络中。
   
2. **Smooth L1 Loss**：
   
   - Smooth L1 Loss 是一种介于 L1 Loss 和 L2 Loss 之间的损失函数，它在接近零的区域采用了平方损失，而在较大差值的区域采用了绝对值损失，因此对异常值有一定的鲁棒性。
   
   - 它的数学表达式如下：
     $$
     \text{SmoothL1Loss} = 
     \begin{cases} 
     0.5 \times (y_i - \hat{y}_i)^2, & \text{if } |y_i - \hat{y}_i| < \text{threshold} \\
     |y_i - \hat{y}_i| - 0.5 \times \text{threshold}, & \text{otherwise}
     \end{cases}
     $$
     这里的 threshold 是一个超参数，用于控制平方损失和绝对值损失之间的切换点。
     
   - Smooth L1 Loss 在异常值处的梯度相对于 L2 Loss 更稳定，因此在训练过程中通常更容易收敛。
   
3. **应用**：
   
   - L2 Loss 通常用于普通的回归任务，但在存在许多异常值的情况下可能会受到影响。
   - Smooth L1 Loss 经常用于对象检测任务中的目标框回归，因为在这些任务中异常值比较常见，同时需要对模型在异常值处的预测有一定的鲁棒性。

总的来说，Smooth L1 Loss 在处理异常值时更稳健，并且在训练过程中梯度更容易处理，但在一些情况下可能会对任务有一些性能影响。选择哪种损失函数取决于具体的应用场景和任务需求。



# PLCC, SROCC, KROCC

[图像质量评价学习笔记02：IQA模型性能评价指标（PLCC、SROCC、KROCC、RMSE）-CSDN博客](https://blog.csdn.net/XieRuily/article/details/121567226)

Scipy库实现：

[Statistical functions for masked arrays (scipy.stats.mstats) — SciPy v1.13.0 Manual](https://docs.scipy.org/doc/scipy/reference/stats.mstats.html#correlation-functions)
